{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = [3, 5, 7, 10, 20, 27, 30]\n",
    "n_docs = 10000\n",
    "textPre_FilePath = \"../data/readme_corpus.txt\"\n",
    "tf_ModelPath = \"../readme_model/readme_tf.pt\"\n",
    "tfidf_ModelPath = \"../readme_model/readme_tfidf.pt\"\n",
    "lda_ModelPath = \"../readme_model/readme_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_data\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "docLst = random.choices(load_data(textPre_FilePath), k=n_docs)\n",
    "len(docLst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['../readme_model/readme_tf.pt']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 150 # Extract 150 feature words\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.3,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(docLst)\n",
    "dump(tf_vectorizer,tf_ModelPath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['../readme_model/readme_tfidf.pt']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(tf)\n",
    "dump(tfidf_vectorizer,tfidf_ModelPath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# load tf and tfidf\n",
    "tfidf_vectorizer = load(tfidf_ModelPath)\n",
    "tf_vectorizer = load(tf_ModelPath)\n",
    "tf = tf_vectorizer.transform(docLst)\n",
    "tfidf = tfidf_vectorizer.transform(tf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import time\n",
    "def train_lda(n_t):\n",
    "    lda = LatentDirichletAllocation(max_iter=100000, learning_method=\"batch\", learning_offset=50, n_jobs=-1, doc_topic_prior=0.04163183877389392,\n",
    "                                        n_components=n_t, topic_word_prior=0.04911253614440047)\n",
    "    start_time = time.time()\n",
    "    lda.fit(tfidf)\n",
    "    end_time = time.time()\n",
    "    print(\"LDA_\" + str(n_t)+\":\")\n",
    "    print(\"time:\", end_time - start_time)\n",
    "    dump(lda,lda_ModelPath + \"lda_\" + str(n_t) + \".pt\")\n",
    "    print(\"Perplexity:\", lda.perplexity(tfidf))\n",
    "    print(\"Score:\", lda.score(tfidf))\n",
    "    print(\"---------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA_10:\n",
      "time: 1.6461155414581299\n",
      "Perplexity: 331.54622352525035\n",
      "Score: -189036.04563153512\n",
      "---------------------------------------------\n",
      "LDA_20:\n",
      "time: 0.39312100410461426\n",
      "Perplexity: 507.45093307709107\n",
      "Score: -202899.4442356089\n",
      "---------------------------------------------\n",
      "LDA_27:\n",
      "time: 0.40181541442871094\n",
      "Perplexity: 636.9816177489308\n",
      "Score: -210304.22105610077\n",
      "---------------------------------------------\n",
      "LDA_30:\n",
      "time: 0.40302205085754395\n",
      "Perplexity: 703.5165299269022\n",
      "Score: -213540.19505860744\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n_t in n_topics:\n",
    "    train_lda(n_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}